{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "successful-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-bunny",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "As defined in the book of Barto and Sutton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tested-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy evaluation sweeps: 1\n",
      "policy evaluation sweeps: 2\n",
      "policy evaluation sweeps: 3\n",
      "policy evaluation sweeps: 4\n",
      "policy evaluation sweeps: 5\n",
      "policy evaluation sweeps: 6\n",
      "policy evaluation sweeps: 7\n",
      "policy evaluation sweeps: 8\n",
      "policy evaluation sweeps: 9\n",
      "policy evaluation sweeps: 10\n",
      "policy evaluation sweeps: 11\n",
      "policy evaluation sweeps: 12\n",
      "policy evaluation sweeps: 13\n",
      "Policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.00962919 0.0088378  0.01811819 0.0086038  0.01337352 0.\n",
      " 0.03847913 0.         0.03181995 0.08390147 0.13750157 0.\n",
      " 0.         0.17000914 0.43332128 0.        ]\n",
      "iteration: 1\n",
      "policy evaluation sweeps: 1\n",
      "policy evaluation sweeps: 2\n",
      "policy evaluation sweeps: 3\n",
      "policy evaluation sweeps: 4\n",
      "policy evaluation sweeps: 5\n",
      "policy evaluation sweeps: 6\n",
      "policy evaluation sweeps: 7\n",
      "Policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "iteration: 2\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(env, policy, discount_factor=1.0, \n",
    "                      theta=1e-3):\n",
    "    V = np.zeros(env.nS)\n",
    "    eval_sweeps = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            v = copy.deepcopy(V[s])\n",
    "            V_s = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, _ in env.P[s][a]:\n",
    "                    V_s += action_prob * prob * \\\n",
    "                        (reward + discount_factor * V[next_state])\n",
    "            V[s] = V_s\n",
    "\n",
    "            delta = max(delta, np.abs(v-V[s]))\n",
    "        eval_sweeps += 1\n",
    "        print(\"policy evaluation sweeps:\", eval_sweeps)\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_improvement(env, policy, V, discount_factor):\n",
    "    def next_actions(env, s, V):\n",
    "        actions = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for prob, next_state, reward, _ in env.P[s][action]:\n",
    "                actions[action] += prob * (reward + discount_factor \\\n",
    "                                      * V[next_state])\n",
    "        return actions\n",
    "    \n",
    "    policy_stable = True\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.argmax(policy[s])\n",
    "        action_values = next_actions(env, s, V)\n",
    "        best_action = np.argmax(action_values)\n",
    "        # Keep the actions sparse\n",
    "        policy[s] = np.eye(env.nA)[best_action]\n",
    "        \n",
    "        if best_action != old_action:\n",
    "            policy_stable = False\n",
    "    return V, policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(env, discount_factor=1.0):\n",
    "    # initial equiprobable policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    iteration = 0\n",
    "    policy_stable = False\n",
    "    \n",
    "    # Value function initiated in policy evaluation\n",
    "    while not policy_stable:\n",
    "        V = policy_evaluation(env, policy, discount_factor)\n",
    "        V, policy, policy_stable = policy_improvement(env, policy, V, discount_factor)\n",
    "        iteration += 1\n",
    "        \n",
    "        print(\"Policy:\", policy)\n",
    "        print(\"Value function:\", V)\n",
    "        print(\"iteration:\", iteration)\n",
    "    return V, policy\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "V, policy = policy_iteration(env, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-nelson",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "By modifying Policy Iteration slightly, the adjustment of taking the max action of the next state to characterize the value function we obtain faster convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "textile-stanford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Iteration: 1\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99 0.   0.   0.99\n",
      " 1.   0.  ]\n",
      "Iteration: 2\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.     0.     0.     0.     0.     0.     0.9801 0.     0.     0.9801\n",
      " 0.99   0.     0.     0.99   1.     0.    ]\n",
      "Iteration: 3\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.         0.         0.970299   0.96059601 0.         0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 4\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.         0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 5\n",
      "policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 6\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env, discount_factor=1.0, \n",
    "                      theta=1e-3):\n",
    "    def next_actions(s, V):\n",
    "        actions = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            for prob, next_state, reward, _ in env.P[s][action]:\n",
    "                actions[action] += prob * (reward + discount_factor \\\n",
    "                                      * V[next_state])\n",
    "        return actions\n",
    "    \n",
    "    # Define an equiprobable policy over all states\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    iteration = 0\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        # We can update our policy in the same loop\n",
    "        for s in range(env.nS):\n",
    "            v = copy.deepcopy(V[s])\n",
    "            v_values = next_actions(s, V)\n",
    "            best_action = np.max(v_values)\n",
    "            V[s] = best_action\n",
    "            \n",
    "            # Update the policy with the best action\n",
    "            policy[s] = np.eye(env.nA)[np.argmax(v_values)]\n",
    "        \n",
    "            delta = max(delta, np.abs(v-V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "        # Print out iterations to compare with Policy\n",
    "        iteration += 1\n",
    "        print(\"Policy:\", policy)\n",
    "        print(\"Value function:\", V)\n",
    "        print(\"iteration:\", iteration)\n",
    "    \n",
    "    return np.array(V), policy\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "V, policy = value_iteration(env, discount_factor=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-christopher",
   "metadata": {},
   "source": [
    "Comparing the iterations required we observe that value iteration even in this trivial example requires less iterations, compared to policy iteration.\n",
    "\n",
    "Furthermore, both policies are equal and yield the same behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "approved-nursery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy evaluation sweeps: 1\n",
      "policy evaluation sweeps: 2\n",
      "policy evaluation sweeps: 3\n",
      "policy evaluation sweeps: 4\n",
      "policy evaluation sweeps: 5\n",
      "policy evaluation sweeps: 6\n",
      "policy evaluation sweeps: 7\n",
      "policy evaluation sweeps: 8\n",
      "policy evaluation sweeps: 9\n",
      "policy evaluation sweeps: 10\n",
      "policy evaluation sweeps: 11\n",
      "policy evaluation sweeps: 12\n",
      "policy evaluation sweeps: 13\n",
      "Policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.00962919 0.0088378  0.01811819 0.0086038  0.01337352 0.\n",
      " 0.03847913 0.         0.03181995 0.08390147 0.13750157 0.\n",
      " 0.         0.17000914 0.43332128 0.        ]\n",
      "iteration: 1\n",
      "policy evaluation sweeps: 1\n",
      "policy evaluation sweeps: 2\n",
      "policy evaluation sweeps: 3\n",
      "policy evaluation sweeps: 4\n",
      "policy evaluation sweeps: 5\n",
      "policy evaluation sweeps: 6\n",
      "policy evaluation sweeps: 7\n",
      "Policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "iteration: 2\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Finished the episode with 1.0                     reward in 5 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In 5 steps and acquired 1.0 reward.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def act_in_environment(env, policy, render=False):\n",
    "    state = env.reset()\n",
    "    episode_rew = 0\n",
    "\n",
    "    for step in range(env._max_episode_steps):\n",
    "        action = np.argmax(policy[state])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        episode_rew += reward\n",
    "\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Finished the episode with {episode_rew} \\\n",
    "                    reward in {step} steps\")\n",
    "            break\n",
    "    return step, episode_rew\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "V, policy = policy_iteration(env, discount_factor=0.99)\n",
    "\n",
    "step, reward = act_in_environment(env, policy, render=True)\n",
    "f\"Solved in {step} steps and acquired {reward} reward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "senior-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "Iteration: 1\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.99 0.   0.   0.99\n",
      " 1.   0.  ]\n",
      "Iteration: 2\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.     0.     0.     0.     0.     0.     0.9801 0.     0.     0.9801\n",
      " 0.99   0.     0.     0.99   1.     0.    ]\n",
      "Iteration: 3\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.         0.         0.970299   0.96059601 0.         0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 4\n",
      "policy: [[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.         0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 5\n",
      "policy: [[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value function: [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n",
      "Iteration: 6\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Finished the episode with 1.0                     reward in 5 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Solved in 5 steps and acquired 1.0 reward.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, policy = value_iteration(env, discount_factor=0.99)\n",
    "step, reward = act_in_environment(env, policy, render=True)\n",
    "f\"Solved in {step} steps and acquired {reward} reward.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-textbook",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('meta': conda)",
   "language": "python",
   "name": "python3710jvsc74a57bd0b1d57bbef129b95556cf4acac245eaf539d69532a51fcbf5e76efb5e83c89ceb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
