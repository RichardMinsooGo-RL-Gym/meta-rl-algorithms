{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, \n",
    "                 n_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(*input_dims, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.parameters(), \n",
    "                       lr=lr)\n",
    "        \n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent(object):\n",
    "    def __init__(self, lr, input_dims, gamma=0.99, n_actions=4):\n",
    "        self.lr = lr \n",
    "        self.gamma = gamma\n",
    "        self.reward_memory = []\n",
    "        self.action_memory = []\n",
    "        \n",
    "        self.policy = PolicyNetwork(self.lr, input_dims, n_actions)\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = T.Tensor([observation]).to(self.policy.device)\n",
    "        probabilities = F.softmax(self.policy.forward(state))\n",
    "        action_probs = T.distributions.Categorical(probabilities)\n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        log_probs = action_probs.log_prob(action)\n",
    "        self.action_memory.append(log_probs)\n",
    "        return action.item() # de-reference with item()\n",
    "\n",
    "    def store_rewards(self, reward):\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        \n",
    "        # G_t = R_t+1 + gamma * R_t+2 + gamma**2 + R_t+3\n",
    "        # G_t = sum from k=0 to k=T { gamma**k * R_t+k+1}\n",
    "        G = np.zeros_like(self.reward_memory)\n",
    "        for t in range(len(self.reward_memory)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            \n",
    "            for k in range(t, len(self.reward_memory)):\n",
    "                G_sum += self.reward_memory[k] * discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "    \n",
    "        G = T.tensor(G, dtype=T.float).to(self.policy.device)\n",
    "        \n",
    "        loss = 0\n",
    "        for g, logprob in zip(G, self.action_memory):\n",
    "            loss += -g * logprob\n",
    "        loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "        \n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(score, x, figure_file=None):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    if figure_file:\n",
    "        plt.savefig(figure_file)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robvangastel/.conda/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -174.03 average score -174.03\n",
      "episode  1 score -340.24 average score -257.14\n",
      "episode  2 score -77.17 average score -197.15\n",
      "episode  3 score -135.03 average score -181.62\n",
      "episode  4 score -238.07 average score -192.91\n",
      "episode  5 score -120.96 average score -180.92\n",
      "episode  6 score -237.16 average score -188.95\n",
      "episode  7 score -281.66 average score -200.54\n",
      "episode  8 score -115.34 average score -191.07\n",
      "episode  9 score -131.56 average score -185.12\n",
      "episode  10 score -155.52 average score -182.43\n",
      "episode  11 score -400.78 average score -200.63\n",
      "episode  12 score -334.23 average score -210.90\n",
      "episode  13 score -321.00 average score -218.77\n",
      "episode  14 score -183.87 average score -216.44\n",
      "episode  15 score -138.99 average score -211.60\n",
      "episode  16 score -125.14 average score -206.51\n",
      "episode  17 score -202.79 average score -206.31\n",
      "episode  18 score -282.05 average score -210.29\n",
      "episode  19 score -382.57 average score -218.91\n",
      "episode  20 score -104.24 average score -213.45\n",
      "episode  21 score -245.05 average score -214.88\n",
      "episode  22 score -99.88 average score -209.88\n",
      "episode  23 score -104.86 average score -205.51\n",
      "episode  24 score -235.77 average score -206.72\n",
      "episode  25 score -120.51 average score -203.40\n",
      "episode  26 score -386.75 average score -210.19\n",
      "episode  27 score -31.29 average score -203.80\n",
      "episode  28 score -117.32 average score -200.82\n",
      "episode  29 score -254.32 average score -202.60\n",
      "episode  30 score -166.14 average score -201.43\n",
      "episode  31 score -436.36 average score -208.77\n",
      "episode  32 score -100.29 average score -205.48\n",
      "episode  33 score -221.74 average score -205.96\n",
      "episode  34 score -292.14 average score -208.42\n",
      "episode  35 score -192.77 average score -207.99\n",
      "episode  36 score -118.93 average score -205.58\n",
      "episode  37 score -423.30 average score -211.31\n",
      "episode  38 score -86.15 average score -208.10\n",
      "episode  39 score -114.49 average score -205.76\n",
      "episode  40 score -80.35 average score -202.70\n",
      "episode  41 score -82.90 average score -199.85\n",
      "episode  42 score -99.13 average score -197.51\n",
      "episode  43 score -126.63 average score -195.90\n",
      "episode  44 score -14.24 average score -191.86\n",
      "episode  45 score -297.96 average score -194.17\n",
      "episode  46 score -173.98 average score -193.74\n",
      "episode  47 score -84.30 average score -191.46\n",
      "episode  48 score -154.44 average score -190.70\n",
      "episode  49 score -149.17 average score -189.87\n",
      "episode  50 score -330.56 average score -192.63\n",
      "episode  51 score -69.11 average score -190.25\n",
      "episode  52 score -270.75 average score -191.77\n",
      "episode  53 score -98.35 average score -190.04\n",
      "episode  54 score -132.55 average score -189.00\n",
      "episode  55 score -386.82 average score -192.53\n",
      "episode  56 score -73.65 average score -190.44\n",
      "episode  57 score -212.58 average score -190.83\n",
      "episode  58 score -81.58 average score -188.98\n",
      "episode  59 score -101.63 average score -187.52\n",
      "episode  60 score -252.90 average score -188.59\n",
      "episode  61 score -51.27 average score -186.38\n",
      "episode  62 score -82.37 average score -184.73\n",
      "episode  63 score -162.77 average score -184.38\n",
      "episode  64 score -98.85 average score -183.07\n",
      "episode  65 score -133.92 average score -182.32\n",
      "episode  66 score -20.18 average score -179.90\n",
      "episode  67 score -313.83 average score -181.87\n",
      "episode  68 score -38.20 average score -179.79\n",
      "episode  69 score 14.53 average score -177.01\n",
      "episode  70 score -284.87 average score -178.53\n",
      "episode  71 score -184.92 average score -178.62\n",
      "episode  72 score -386.58 average score -181.47\n",
      "episode  73 score -347.98 average score -183.72\n",
      "episode  74 score -368.76 average score -186.19\n",
      "episode  75 score -316.00 average score -187.90\n",
      "episode  76 score -354.01 average score -190.05\n",
      "episode  77 score -473.90 average score -193.69\n",
      "episode  78 score -137.17 average score -192.98\n",
      "episode  79 score -80.04 average score -191.56\n",
      "episode  80 score -84.47 average score -190.24\n",
      "episode  81 score -96.16 average score -189.09\n",
      "episode  82 score -114.46 average score -188.20\n",
      "episode  83 score -335.93 average score -189.95\n",
      "episode  84 score -119.33 average score -189.12\n",
      "episode  85 score -378.56 average score -191.33\n",
      "episode  86 score -103.78 average score -190.32\n",
      "episode  87 score -318.69 average score -191.78\n",
      "episode  88 score -111.18 average score -190.87\n",
      "episode  89 score -249.98 average score -191.53\n",
      "episode  90 score -58.69 average score -190.07\n",
      "episode  91 score -487.01 average score -193.30\n",
      "episode  92 score -97.84 average score -192.27\n",
      "episode  93 score -342.19 average score -193.87\n",
      "episode  94 score -76.35 average score -192.63\n",
      "episode  95 score -81.28 average score -191.47\n",
      "episode  96 score -79.17 average score -190.31\n",
      "episode  97 score -204.74 average score -190.46\n",
      "episode  98 score -226.07 average score -190.82\n",
      "episode  99 score -161.98 average score -190.53\n",
      "episode  100 score -143.58 average score -190.23\n",
      "episode  101 score -386.50 average score -190.69\n",
      "episode  102 score -85.70 average score -190.77\n",
      "episode  103 score -259.39 average score -192.02\n",
      "episode  104 score -27.66 average score -189.91\n",
      "episode  105 score -90.04 average score -189.60\n",
      "episode  106 score -163.88 average score -188.87\n",
      "episode  107 score -174.96 average score -187.80\n",
      "episode  108 score -214.55 average score -188.80\n",
      "episode  109 score -215.19 average score -189.63\n",
      "episode  110 score -333.77 average score -191.41\n",
      "episode  111 score -99.16 average score -188.40\n",
      "episode  112 score -493.17 average score -189.99\n",
      "episode  113 score -120.51 average score -187.98\n",
      "episode  114 score -307.70 average score -189.22\n",
      "episode  115 score -240.23 average score -190.23\n",
      "episode  116 score -172.41 average score -190.71\n",
      "episode  117 score -442.97 average score -193.11\n",
      "episode  118 score -116.90 average score -191.46\n",
      "episode  119 score -96.06 average score -188.59\n",
      "episode  120 score -186.25 average score -189.41\n",
      "episode  121 score -85.92 average score -187.82\n",
      "episode  122 score -258.46 average score -189.41\n",
      "episode  123 score -286.26 average score -191.22\n",
      "episode  124 score -216.16 average score -191.02\n",
      "episode  125 score -146.36 average score -191.28\n",
      "episode  126 score -111.71 average score -188.53\n",
      "episode  127 score -187.00 average score -190.09\n",
      "episode  128 score -135.26 average score -190.27\n",
      "episode  129 score -117.88 average score -188.90\n",
      "episode  130 score -48.81 average score -187.73\n",
      "episode  131 score -130.46 average score -184.67\n",
      "episode  132 score -551.74 average score -189.19\n",
      "episode  133 score -325.53 average score -190.22\n",
      "episode  134 score -428.72 average score -191.59\n",
      "episode  135 score -520.01 average score -194.86\n",
      "episode  136 score -92.86 average score -194.60\n",
      "episode  137 score -471.57 average score -195.08\n",
      "episode  138 score -94.47 average score -195.17\n",
      "episode  139 score -483.09 average score -198.85\n",
      "episode  140 score -145.82 average score -199.51\n",
      "episode  141 score -121.13 average score -199.89\n",
      "episode  142 score -98.40 average score -199.88\n",
      "episode  143 score 84.04 average score -197.78\n",
      "episode  144 score -168.23 average score -199.32\n",
      "episode  145 score -157.81 average score -197.92\n",
      "episode  146 score -42.17 average score -196.60\n",
      "episode  147 score -96.12 average score -196.72\n",
      "episode  148 score -102.24 average score -196.19\n",
      "episode  149 score -222.14 average score -196.92\n",
      "episode  150 score -67.47 average score -194.29\n",
      "episode  151 score -176.82 average score -195.37\n",
      "episode  152 score -137.25 average score -194.03\n",
      "episode  153 score -101.12 average score -194.06\n",
      "episode  154 score -31.12 average score -193.05\n",
      "episode  155 score -166.59 average score -190.85\n",
      "episode  156 score -71.43 average score -190.82\n",
      "episode  157 score -56.46 average score -189.26\n",
      "episode  158 score -103.04 average score -189.48\n",
      "episode  159 score -344.70 average score -191.91\n",
      "episode  160 score 0.31 average score -189.37\n",
      "episode  161 score -162.30 average score -190.49\n",
      "episode  162 score -127.46 average score -190.94\n",
      "episode  163 score -145.26 average score -190.76\n",
      "episode  164 score -131.70 average score -191.09\n",
      "episode  165 score -78.51 average score -190.54\n",
      "episode  166 score -394.32 average score -194.28\n",
      "episode  167 score -196.22 average score -193.10\n",
      "episode  168 score -410.38 average score -196.82\n",
      "episode  169 score -67.71 average score -197.64\n",
      "episode  170 score -87.86 average score -195.67\n",
      "episode  171 score -279.85 average score -196.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  172 score -261.35 average score -195.37\n",
      "episode  173 score -506.94 average score -196.96\n",
      "episode  174 score -90.38 average score -194.18\n",
      "episode  175 score -242.56 average score -193.44\n",
      "episode  176 score -247.22 average score -192.38\n",
      "episode  177 score -188.00 average score -189.52\n",
      "episode  178 score -128.43 average score -189.43\n",
      "episode  179 score -449.16 average score -193.12\n",
      "episode  180 score -92.85 average score -193.20\n",
      "episode  181 score -153.03 average score -193.77\n",
      "episode  182 score -143.35 average score -194.06\n",
      "episode  183 score -302.47 average score -193.73\n",
      "episode  184 score -102.51 average score -193.56\n",
      "episode  185 score -175.47 average score -191.53\n",
      "episode  186 score -399.15 average score -194.48\n",
      "episode  187 score -92.97 average score -192.22\n",
      "episode  188 score -147.90 average score -192.59\n",
      "episode  189 score -104.05 average score -191.13\n",
      "episode  190 score -9.71 average score -190.64\n",
      "episode  191 score -161.26 average score -187.38\n",
      "episode  192 score -133.41 average score -187.74\n",
      "episode  193 score -14.99 average score -184.47\n",
      "episode  194 score -59.78 average score -184.30\n",
      "episode  195 score -134.05 average score -184.83\n",
      "episode  196 score -275.88 average score -186.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-81c190abd3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-50e95c8c4375>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# de-reference with item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_pmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "n_games = 3000\n",
    "agent = PolicyGradientAgent(gamma=0.99, lr=0.0005, input_dims=[8],\n",
    "                            n_actions=4)\n",
    "\n",
    "fname = 'REINFORCE_' + 'lunar_lunar_lr' + str(agent.lr) + '_' \\\n",
    "        + str(n_games) + 'games'\n",
    "figure_file = 'plots/' + fname + '.png'\n",
    "\n",
    "scores = []\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.store_rewards(reward)\n",
    "        observation = observation_\n",
    "    agent.learn()\n",
    "    scores.append(score)\n",
    "\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    print('episode ', i, 'score %.2f' % score,\n",
    "            'average score %.2f' % avg_score)\n",
    "\n",
    "x = [i+1 for i in range(len(scores))]\n",
    "plot_learning_curve(scores, x, figure_file)\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
