{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "verbal-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-purple",
   "metadata": {},
   "source": [
    "We can consider using a *baseline* $b$ in our result, which was originally introduced by [Williams, 1992](https://link.springer.com/article/10.1007/BF00992696). The paper in which the REINFORCE algorithm is introduced. \n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})=\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{t=1}^{H} \\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}\\left(\\mathbf{a}_{t}^{(i)} \\mid \\mathbf{s}_{t}^{(i)}\\right)\\left[\\sum_{k=t}^{H} \\gamma^{k} r\\left(\\mathbf{s}_{k}^{(i)}, \\mathbf{a}_{k}^{(i)}\\right)-b\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "english-wings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32),\n",
       " Discrete(2),\n",
       " 500)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "env.observation_space, env.action_space, env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "willing-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(obs_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, act_dim)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class Agent:\n",
    "    def __init__(self, env, gamma=0.99, lr=3e-2, hidden_size=64):\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.act_dim = env.action_space.n\n",
    "        self.max_ep_len = env._max_episode_steps\n",
    "        \n",
    "        self.log_pi = Model(self.obs_dim, self.act_dim)\n",
    "        self.opt = optim.Adam(self.log_pi.parameters(), lr=lr)\n",
    "    \n",
    "    def update_policy(self, act, obs, baseline):\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        logp = self.get_policy(obs).log_prob(act)\n",
    "        batch_loss = -(logp * baseline).mean()  \n",
    "        \n",
    "        batch_loss.backward()\n",
    "        self.opt.step()\n",
    "        return batch_loss\n",
    "        \n",
    "    def get_policy(self, obs):\n",
    "        logits = self.log_pi(obs)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def get_action(self, obs, deterministic = False):\n",
    "        # TODO: Setup deterministic get_action\n",
    "        return self.get_policy(obs).sample().item()\n",
    "    \n",
    "    def sample_batch(self, batch_size=5000):\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "        \n",
    "        done = False\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        while True:\n",
    "            batch_obs.append(o)\n",
    "            \n",
    "            a = self.get_action(torch.as_tensor(o, dtype=torch.float32))\n",
    "            o, r, done, _ = env.step(a)\n",
    "            \n",
    "            # save action, reward\n",
    "            batch_acts.append(a)\n",
    "            ep_rews.append(r)\n",
    "            \n",
    "            if done or(ep_len == self.max_ep_len):\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                # batch_weights += [ep_ret] * ep_len\n",
    "                \n",
    "                # Reward-to-go\n",
    "                # batch_weights += list(reward_to_go(ep_rews))\n",
    "                \n",
    "                # Discounted reward\n",
    "                batch_weights += list(discounted_reward(ep_rews))\n",
    "                \n",
    "                # reset episode-specific variables\n",
    "                o, done, ep_rews = env.reset(), False, []\n",
    "                \n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "        \n",
    "        batch_loss = self.update_policy(\n",
    "            torch.as_tensor(batch_acts, dtype=torch.float32),\n",
    "            torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "            torch.as_tensor(batch_weights, dtype=torch.float32))\n",
    "  \n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "    \n",
    "    def train(self, epochs=50, batch_size=5000):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            batch_loss, batch_rets, batch_lens = self.sample_batch(\n",
    "                batch_size=batch_size)\n",
    "            print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f' %\n",
    "                  (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 7.111 \t return: 19.761 \t ep_len: 19.761\n",
      "epoch:   1 \t loss: 10.994 \t return: 40.500 \t ep_len: 40.500\n",
      "epoch:   2 \t loss: 10.426 \t return: 43.216 \t ep_len: 43.216\n",
      "epoch:   3 \t loss: 11.518 \t return: 59.106 \t ep_len: 59.106\n",
      "epoch:   4 \t loss: 12.210 \t return: 80.556 \t ep_len: 80.556\n",
      "epoch:   5 \t loss: 11.339 \t return: 83.967 \t ep_len: 83.967\n",
      "epoch:   6 \t loss: 11.193 \t return: 85.661 \t ep_len: 85.661\n",
      "epoch:   7 \t loss: 11.428 \t return: 94.717 \t ep_len: 94.717\n",
      "epoch:   8 \t loss: 11.613 \t return: 125.146 \t ep_len: 125.146\n",
      "epoch:   9 \t loss: 11.066 \t return: 194.000 \t ep_len: 194.000\n",
      "epoch:  10 \t loss: 11.013 \t return: 220.435 \t ep_len: 220.435\n",
      "epoch:  11 \t loss: 11.561 \t return: 184.500 \t ep_len: 184.500\n",
      "epoch:  12 \t loss: 10.930 \t return: 213.375 \t ep_len: 213.375\n",
      "epoch:  13 \t loss: 10.561 \t return: 244.429 \t ep_len: 244.429\n",
      "epoch:  14 \t loss: 10.036 \t return: 268.842 \t ep_len: 268.842\n",
      "epoch:  15 \t loss: 10.219 \t return: 255.450 \t ep_len: 255.450\n",
      "epoch:  16 \t loss: 9.370 \t return: 279.105 \t ep_len: 279.105\n",
      "epoch:  17 \t loss: 8.549 \t return: 286.056 \t ep_len: 286.056\n",
      "epoch:  18 \t loss: 7.163 \t return: 282.526 \t ep_len: 282.526\n",
      "epoch:  19 \t loss: 6.435 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  20 \t loss: 6.614 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  21 \t loss: 6.262 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  22 \t loss: 6.302 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  23 \t loss: 6.346 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  24 \t loss: 6.001 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  25 \t loss: 5.987 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  26 \t loss: 6.212 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  27 \t loss: 6.013 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  28 \t loss: 5.618 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  29 \t loss: 5.877 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  30 \t loss: 5.582 \t return: 287.632 \t ep_len: 287.632\n",
      "epoch:  31 \t loss: 5.469 \t return: 284.105 \t ep_len: 284.105\n",
      "epoch:  32 \t loss: 6.445 \t return: 300.111 \t ep_len: 300.111\n",
      "epoch:  33 \t loss: 6.699 \t return: 310.294 \t ep_len: 310.294\n",
      "epoch:  34 \t loss: 7.622 \t return: 253.571 \t ep_len: 253.571\n",
      "epoch:  35 \t loss: 5.918 \t return: 266.158 \t ep_len: 266.158\n",
      "epoch:  36 \t loss: 7.357 \t return: 251.950 \t ep_len: 251.950\n",
      "epoch:  37 \t loss: 7.563 \t return: 284.556 \t ep_len: 284.556\n",
      "epoch:  38 \t loss: 6.433 \t return: 381.214 \t ep_len: 381.214\n",
      "epoch:  39 \t loss: 6.401 \t return: 392.462 \t ep_len: 392.462\n",
      "epoch:  40 \t loss: 6.119 \t return: 420.417 \t ep_len: 420.417\n",
      "epoch:  41 \t loss: 6.822 \t return: 368.000 \t ep_len: 368.000\n",
      "epoch:  42 \t loss: 6.081 \t return: 418.000 \t ep_len: 418.000\n",
      "epoch:  43 \t loss: 5.975 \t return: 419.917 \t ep_len: 419.917\n",
      "epoch:  44 \t loss: 6.078 \t return: 454.909 \t ep_len: 454.909\n",
      "epoch:  45 \t loss: 5.856 \t return: 263.211 \t ep_len: 263.211\n",
      "epoch:  46 \t loss: 5.615 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  47 \t loss: 5.959 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  48 \t loss: 5.924 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  49 \t loss: 6.194 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  50 \t loss: 6.159 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  51 \t loss: 6.563 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  52 \t loss: 6.587 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  53 \t loss: 6.336 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  54 \t loss: 6.543 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  55 \t loss: 6.740 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  56 \t loss: 6.569 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  57 \t loss: 6.974 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  58 \t loss: 7.165 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  59 \t loss: 7.788 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  60 \t loss: 7.562 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  61 \t loss: 7.912 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  62 \t loss: 8.310 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  63 \t loss: 8.495 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  64 \t loss: 8.680 \t return: 263.632 \t ep_len: 263.632\n",
      "epoch:  65 \t loss: 8.979 \t return: 263.632 \t ep_len: 263.632\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)\n",
    "agent.train(epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "departmental-savannah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 17, 16, 14, 13, 10])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Trick introduced in DeepRL bootcamp, lecture 5\n",
    "def reward_to_go(rews):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "reward_to_go([10, 1, 2, 1, 3, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "rural-holocaust",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 12, 12, 11, 11,  9])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discounted_reward(rews, gamma=0.99):\n",
    "    n = len(rews)\n",
    "    rtgs = np.zeros_like(rews)\n",
    "    for i in reversed(range(n)):\n",
    "        rtgs[i] = gamma**i * rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "    return rtgs\n",
    "\n",
    "\n",
    "discounted_reward([10, 1, 2, 1, 3, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "forty-shoulder",
   "metadata": {},
   "source": [
    "Finally there are several possibilities for picking a baseline, namely,\n",
    "$\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})=\\mathbb{E}_{\\mathbf{s}_{t}, \\mathbf{a}_{t} \\pi_{\\boldsymbol{\\theta}}(.)}\\left[\\nabla_{\\boldsymbol{\\theta}} \\log \\pi_{\\boldsymbol{\\theta}}\\left(\\mathbf{a}_{t}^{(i)} \\mid \\mathbf{s}_{t}^{(i)}\\right)\\right] \\psi_{t} \\text { where } \\psi_{t} \\text { can be: }$\n",
    "\n",
    "* $\\sum_{t=0}^{H} \\gamma^{t} r_{t}:$ total (discounted) reward of trajectory\n",
    "* $\\sum_{k=t}^{H} \\gamma^{k-t} r_{k}:$ sum of rewards after $\\mathbf{a}_{t}$\n",
    "* $\\sum_{k=t}^{H} \\gamma^{k-t} r_{k}-b\\left(\\mathbf{s}_{t}\\right):$ sum of rewards after $\\mathbf{a}_{t}$ with baseline\n",
    "* $\\delta_{t}=r_{t}+\\gamma V^{\\pi}\\left(\\mathbf{s}_{t+1}\\right)-V^{\\pi}\\left(\\mathbf{s}_{t}\\right):$ error, with $V^{\\pi}\\left(\\mathbf{s}_{t}\\right)=\\mathbb{E}_{\\mathbf{a}_{t}}\\left[\\sum_{k=0}^{H} \\gamma^{k} r_{t+l}\\right]$\n",
    "* $\\hat{Q}_{\\phi}^{\\pi_{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=\\mathbb{E}_{a_{t+1}}\\left[\\sum_{k=0}^{H} \\gamma^{k} r_{t+l}\\right]:$ action-value function\n",
    "* $\\hat{A}_{\\phi}^{\\pi_{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)=\\hat{Q}_{\\phi}^{\\pi_{\\theta}}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)-\\hat{V}_{\\phi}^{\\pi_{\\theta}}\\left(\\mathbf{s}_{t}\\right)=\\mathbb{E}\\left[\\delta_{t}\\right]$, advantage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-identification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-circle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-elephant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
