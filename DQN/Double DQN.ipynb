{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN\n",
    "# https://arxiv.org/abs/1509.06461\n",
    "# Double Q-learning was first introduced in 2010 by van Hasselt.\n",
    "\n",
    "# Double Q-Learning\n",
    "# https://papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 0.80, [last 100]: 0.08, [all]: 8.00                       \n",
      "epsilon: 0.89, frames_total: 8\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 21.30, [last 100]: 2.21, [all]: 20.09                       \n",
      "epsilon: 0.58, frames_total: 221\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 13.40, [last 100]: 3.55, [all]: 16.90                       \n",
      "epsilon: 0.45, frames_total: 355\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 10.80, [last 100]: 4.63, [all]: 14.94                       \n",
      "epsilon: 0.36, frames_total: 463\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 10.60, [last 100]: 5.69, [all]: 13.88                       \n",
      "epsilon: 0.30, frames_total: 569\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 12.60, [last 100]: 6.95, [all]: 13.63                       \n",
      "epsilon: 0.23, frames_total: 695\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 11.90, [last 100]: 8.14, [all]: 13.34                       \n",
      "epsilon: 0.18, frames_total: 814\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 11.00, [last 100]: 9.24, [all]: 13.01                       \n",
      "epsilon: 0.15, frames_total: 924\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 11.50, [last 100]: 10.39, [all]: 12.83                       \n",
      "epsilon: 0.12, frames_total: 1039\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 11.10, [last 100]: 11.50, [all]: 12.64                       \n",
      "epsilon: 0.10, frames_total: 1150\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 10.40, [last 100]: 12.46, [all]: 12.42                       \n",
      "epsilon: 0.08, frames_total: 1254\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 11.10, [last 100]: 11.44, [all]: 12.30                       \n",
      "epsilon: 0.07, frames_total: 1365\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 11.40, [last 100]: 11.24, [all]: 12.22                       \n",
      "epsilon: 0.06, frames_total: 1479\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 10.40, [last 100]: 11.20, [all]: 12.08                       \n",
      "epsilon: 0.05, frames_total: 1583\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 10.80, [last 100]: 11.22, [all]: 11.99                       \n",
      "epsilon: 0.04, frames_total: 1691\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 12.00, [last 100]: 11.16, [all]: 11.99                       \n",
      "epsilon: 0.03, frames_total: 1811\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 13.50, [last 100]: 11.32, [all]: 12.09                       \n",
      "epsilon: 0.03, frames_total: 1946\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 14.70, [last 100]: 11.69, [all]: 12.24                       \n",
      "epsilon: 0.02, frames_total: 2093\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 14.70, [last 100]: 12.01, [all]: 12.38                       \n",
      "epsilon: 0.02, frames_total: 2240\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 16.00, [last 100]: 12.50, [all]: 12.57                       \n",
      "epsilon: 0.02, frames_total: 2400\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 15.90, [last 100]: 13.05, [all]: 12.73                       \n",
      "epsilon: 0.02, frames_total: 2559\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 16.80, [last 100]: 13.62, [all]: 12.92                       \n",
      "epsilon: 0.01, frames_total: 2727\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 18.50, [last 100]: 14.33, [all]: 13.18                       \n",
      "epsilon: 0.01, frames_total: 2912\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 24.20, [last 100]: 15.71, [all]: 13.65                       \n",
      "epsilon: 0.01, frames_total: 3154\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 44.50, [last 100]: 19.08, [all]: 14.93                       \n",
      "epsilon: 0.01, frames_total: 3599\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 48.60, [last 100]: 22.74, [all]: 16.27                       \n",
      "epsilon: 0.01, frames_total: 4085\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 76.30, [last 100]: 29.02, [all]: 18.57                       \n",
      "epsilon: 0.01, frames_total: 4848\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 45.10, [last 100]: 32.06, [all]: 19.55                       \n",
      "epsilon: 0.01, frames_total: 5299\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 65.30, [last 100]: 37.12, [all]: 21.18                       \n",
      "epsilon: 0.01, frames_total: 5952\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 55.10, [last 100]: 41.03, [all]: 22.35                       \n",
      "epsilon: 0.01, frames_total: 6503\n",
      "Elapsed time:  00:00:07\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 52.90, [last 100]: 44.73, [all]: 23.36                       \n",
      "epsilon: 0.01, frames_total: 7032\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 88.10, [last 100]: 51.86, [all]: 25.44                       \n",
      "epsilon: 0.01, frames_total: 7913\n",
      "Elapsed time:  00:00:09\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 155.60, [last 100]: 65.57, [all]: 29.50                       \n",
      "epsilon: 0.01, frames_total: 9469\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 287.50, [last 100]: 91.90, [all]: 37.29                       \n",
      "epsilon: 0.01, frames_total: 12344\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 425.00, [last 100]: 129.95, [all]: 48.66                       \n",
      "epsilon: 0.01, frames_total: 16594\n",
      "Elapsed time:  00:00:18\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 360.50, [last 100]: 161.14, [all]: 57.55                       \n",
      "epsilon: 0.01, frames_total: 20199\n",
      "Elapsed time:  00:00:22\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 306.80, [last 100]: 184.19, [all]: 64.45                       \n",
      "epsilon: 0.01, frames_total: 23267\n",
      "Elapsed time:  00:00:25\n",
      "SOLVED! After 365 episodes \n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 256.70, [last 100]: 205.35, [all]: 69.63                       \n",
      "epsilon: 0.01, frames_total: 25834\n",
      "Elapsed time:  00:00:27\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 293.10, [last 100]: 228.13, [all]: 75.50                       \n",
      "epsilon: 0.01, frames_total: 28765\n",
      "Elapsed time:  00:00:30\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 306.50, [last 100]: 253.27, [all]: 81.41                       \n",
      "epsilon: 0.01, frames_total: 31830\n",
      "Elapsed time:  00:00:33\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 87.48\n",
      "Average reward (last 100 episodes): 280.66\n",
      "Solved after 365 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 400\n",
    "gamma = 0.999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "# stable vs faster convergence trade-off\n",
    "update_target_frequency = 500\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "clip_error = True\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "\n",
    "        return output2\n",
    "\n",
    "    \n",
    "# Memory for experience replay\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else: \n",
    "            self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1 ) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Update params target net\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "        \n",
    "    def select_action(self,state, epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_index = torch.max(new_state_indexes, 1)[1]\n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_index.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "\n",
    "        \n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values\n",
    "\n",
    "        \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "#         print(self.nn(state).size())\n",
    "#         print(action.unsqueeze(1).size())\n",
    "#         print(predicted_value.size())\n",
    "#         print(target_value.size())\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "            \n",
    "        self.update_target_counter += 1\n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
