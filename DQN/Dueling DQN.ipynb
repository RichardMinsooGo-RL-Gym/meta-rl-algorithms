{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1511.06581\n",
    "# Dueling DQN\n",
    "\n",
    "# Interesting difference between advantage \n",
    "# and value is noted in this paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.30, [last 100]: 0.13, [all]: 13.00                       \n",
      "epsilon: 0.88, frames_total: 13\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 16.70, [last 100]: 1.80, [all]: 16.36                       \n",
      "epsilon: 0.63, frames_total: 180\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 23.20, [last 100]: 4.12, [all]: 19.62                       \n",
      "epsilon: 0.40, frames_total: 412\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 36.90, [last 100]: 7.81, [all]: 25.19                       \n",
      "epsilon: 0.20, frames_total: 781\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 47.50, [last 100]: 12.56, [all]: 30.63                       \n",
      "epsilon: 0.08, frames_total: 1256\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 41.10, [last 100]: 16.67, [all]: 32.69                       \n",
      "epsilon: 0.04, frames_total: 1667\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 48.50, [last 100]: 21.52, [all]: 35.28                       \n",
      "epsilon: 0.02, frames_total: 2152\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 38.10, [last 100]: 25.33, [all]: 35.68                       \n",
      "epsilon: 0.02, frames_total: 2533\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 40.70, [last 100]: 29.40, [all]: 36.30                       \n",
      "epsilon: 0.01, frames_total: 2940\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 39.00, [last 100]: 33.30, [all]: 36.59                       \n",
      "epsilon: 0.01, frames_total: 3330\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 55.60, [last 100]: 38.73, [all]: 38.48                       \n",
      "epsilon: 0.01, frames_total: 3886\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 87.10, [last 100]: 45.77, [all]: 42.86                       \n",
      "epsilon: 0.01, frames_total: 4757\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 191.70, [last 100]: 62.62, [all]: 55.16                       \n",
      "epsilon: 0.01, frames_total: 6674\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 78.93, [all]: 66.21                       \n",
      "epsilon: 0.01, frames_total: 8674\n",
      "Elapsed time:  00:00:11\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 94.18, [all]: 75.70                       \n",
      "epsilon: 0.01, frames_total: 10674\n",
      "Elapsed time:  00:00:14\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 110.07, [all]: 83.93                       \n",
      "epsilon: 0.01, frames_total: 12674\n",
      "Elapsed time:  00:00:17\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 125.22, [all]: 91.14                       \n",
      "epsilon: 0.01, frames_total: 14674\n",
      "Elapsed time:  00:00:19\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 141.41, [all]: 97.51                       \n",
      "epsilon: 0.01, frames_total: 16674\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 157.34, [all]: 103.17                       \n",
      "epsilon: 0.01, frames_total: 18674\n",
      "Elapsed time:  00:00:26\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 173.44, [all]: 108.24                       \n",
      "epsilon: 0.01, frames_total: 20674\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 187.88, [all]: 112.81                       \n",
      "epsilon: 0.01, frames_total: 22674\n",
      "Elapsed time:  00:00:33\n",
      "SOLVED! After 206 episodes \n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 199.17, [all]: 116.94                       \n",
      "epsilon: 0.01, frames_total: 24674\n",
      "Elapsed time:  00:00:36\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 120.70                       \n",
      "epsilon: 0.01, frames_total: 26674\n",
      "Elapsed time:  00:00:40\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 124.13                       \n",
      "epsilon: 0.01, frames_total: 28674\n",
      "Elapsed time:  00:00:43\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 127.28                       \n",
      "epsilon: 0.01, frames_total: 30674\n",
      "Elapsed time:  00:00:47\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 130.18                       \n",
      "epsilon: 0.01, frames_total: 32674\n",
      "Elapsed time:  00:00:50\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 132.85                       \n",
      "epsilon: 0.01, frames_total: 34674\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 135.33                       \n",
      "epsilon: 0.01, frames_total: 36674\n",
      "Elapsed time:  00:00:57\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 137.63                       \n",
      "epsilon: 0.01, frames_total: 38674\n",
      "Elapsed time:  00:01:00\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 139.77                       \n",
      "epsilon: 0.01, frames_total: 40674\n",
      "Elapsed time:  00:01:04\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 141.77                       \n",
      "epsilon: 0.01, frames_total: 42674\n",
      "Elapsed time:  00:01:07\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 143.65                       \n",
      "epsilon: 0.01, frames_total: 44674\n",
      "Elapsed time:  00:01:10\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 145.40                       \n",
      "epsilon: 0.01, frames_total: 46674\n",
      "Elapsed time:  00:01:14\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 147.05                       \n",
      "epsilon: 0.01, frames_total: 48674\n",
      "Elapsed time:  00:01:17\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 148.60                       \n",
      "epsilon: 0.01, frames_total: 50674\n",
      "Elapsed time:  00:01:20\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 150.07                       \n",
      "epsilon: 0.01, frames_total: 52674\n",
      "Elapsed time:  00:01:23\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 151.45                       \n",
      "epsilon: 0.01, frames_total: 54674\n",
      "Elapsed time:  00:01:26\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 152.76                       \n",
      "epsilon: 0.01, frames_total: 56674\n",
      "Elapsed time:  00:01:29\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 154.00                       \n",
      "epsilon: 0.01, frames_total: 58674\n",
      "Elapsed time:  00:01:32\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 155.18                       \n",
      "epsilon: 0.01, frames_total: 60674\n",
      "Elapsed time:  00:01:35\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 156.29                       \n",
      "epsilon: 0.01, frames_total: 62674\n",
      "Elapsed time:  00:01:38\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 157.36                       \n",
      "epsilon: 0.01, frames_total: 64674\n",
      "Elapsed time:  00:01:41\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 158.37                       \n",
      "epsilon: 0.01, frames_total: 66674\n",
      "Elapsed time:  00:01:44\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 159.34                       \n",
      "epsilon: 0.01, frames_total: 68674\n",
      "Elapsed time:  00:01:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 160.26                       \n",
      "epsilon: 0.01, frames_total: 70674\n",
      "Elapsed time:  00:01:49\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 161.14                       \n",
      "epsilon: 0.01, frames_total: 72674\n",
      "Elapsed time:  00:01:52\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 161.98                       \n",
      "epsilon: 0.01, frames_total: 74674\n",
      "Elapsed time:  00:01:55\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 162.79                       \n",
      "epsilon: 0.01, frames_total: 76674\n",
      "Elapsed time:  00:01:58\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 163.56                       \n",
      "epsilon: 0.01, frames_total: 78674\n",
      "Elapsed time:  00:02:01\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 200.00, [last 100]: 200.00, [all]: 164.31                       \n",
      "epsilon: 0.01, frames_total: 80674\n",
      "Elapsed time:  00:02:05\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 164.95\n",
      "Average reward (last 100 episodes): 200.00\n",
      "Solved after 206 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.001\n",
    "num_episodes = 500\n",
    "gamma = 0.9999\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "double_dqn = True\n",
    "\n",
    "update_target_frequency = 1500\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = False\n",
    "\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory[self.position] = transition\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        \n",
    "        self.advantage = nn.Linear(hidden_layer,number_of_outputs)\n",
    "        self.value = nn.Linear(hidden_layer,1)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        \n",
    "        output_advantage = self.advantage(output1)\n",
    "        output_value = self.value(output1)\n",
    "        \n",
    "        output_final = output_value + output_advantage - output_advantage.mean()\n",
    "        \n",
    "        return output_final\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "    def select_action(self,state,epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        \n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "\n",
    "        if double_dqn:\n",
    "            new_state_indexes = self.nn(new_state).detach()\n",
    "            max_new_state_indexes = torch.max(new_state_indexes, 1)[1]  \n",
    "            \n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = new_state_values.gather(1, max_new_state_indexes.unsqueeze(1)).squeeze(1)\n",
    "        else:\n",
    "            new_state_values = self.target_nn(new_state).detach()\n",
    "            max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        \n",
    "        \n",
    "        target_value = reward + ( 1 - done ) * gamma * max_new_state_values\n",
    "  \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "        \n",
    "        self.update_target_counter += 1\n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green', width=5)\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
