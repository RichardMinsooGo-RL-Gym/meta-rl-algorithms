{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 2.90, [last 100]: 0.29, [all]: 29.00                       \n",
      "epsilon: 0.85, frames_total: 29\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 50.40, [last 100]: 5.33, [all]: 48.45                       \n",
      "epsilon: 0.32, frames_total: 533\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 192.10, [last 100]: 24.54, [all]: 116.86                       \n",
      "epsilon: 0.03, frames_total: 2454\n",
      "Elapsed time:  00:00:02\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 307.00, [last 100]: 55.24, [all]: 178.19                       \n",
      "epsilon: 0.02, frames_total: 5524\n",
      "Elapsed time:  00:00:04\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 470.10, [last 100]: 102.25, [all]: 249.39                       \n",
      "epsilon: 0.02, frames_total: 10225\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 492.50, [last 100]: 151.50, [all]: 297.06                       \n",
      "epsilon: 0.02, frames_total: 15150\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 403.20, [last 100]: 191.82, [all]: 314.46                       \n",
      "epsilon: 0.02, frames_total: 19182\n",
      "Elapsed time:  00:00:16\n",
      "SOLVED! After 61 episodes \n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 327.40, [last 100]: 224.56, [all]: 316.28                       \n",
      "epsilon: 0.02, frames_total: 22456\n",
      "Elapsed time:  00:00:19\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 428.10, [last 100]: 267.37, [all]: 330.09                       \n",
      "epsilon: 0.02, frames_total: 26737\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 308.80, [last 100]: 298.25, [all]: 327.75                       \n",
      "epsilon: 0.02, frames_total: 29825\n",
      "Elapsed time:  00:00:26\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 388.60, [last 100]: 336.82, [all]: 333.77                       \n",
      "epsilon: 0.02, frames_total: 33711\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 398.40, [last 100]: 371.62, [all]: 339.59                       \n",
      "epsilon: 0.02, frames_total: 37695\n",
      "Elapsed time:  00:00:33\n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 448.30, [last 100]: 397.24, [all]: 348.58                       \n",
      "epsilon: 0.02, frames_total: 42178\n",
      "Elapsed time:  00:00:37\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 456.60, [last 100]: 412.20, [all]: 356.82                       \n",
      "epsilon: 0.02, frames_total: 46744\n",
      "Elapsed time:  00:00:41\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 407.70, [last 100]: 405.96, [all]: 360.43                       \n",
      "epsilon: 0.02, frames_total: 50821\n",
      "Elapsed time:  00:00:44\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 401.60, [last 100]: 396.87, [all]: 363.16                       \n",
      "epsilon: 0.02, frames_total: 54837\n",
      "Elapsed time:  00:00:48\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 139.70, [last 100]: 370.52, [all]: 349.28                       \n",
      "epsilon: 0.02, frames_total: 56234\n",
      "Elapsed time:  00:00:49\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 336.20, [last 100]: 371.40, [all]: 348.51                       \n",
      "epsilon: 0.02, frames_total: 59596\n",
      "Elapsed time:  00:00:52\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 103.00, [last 100]: 338.89, [all]: 334.95                       \n",
      "epsilon: 0.02, frames_total: 60626\n",
      "Elapsed time:  00:00:53\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 10.50, [last 100]: 309.06, [all]: 317.96                       \n",
      "epsilon: 0.02, frames_total: 60731\n",
      "Elapsed time:  00:00:53\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 144.10, [last 100]: 284.61, [all]: 309.31                       \n",
      "epsilon: 0.02, frames_total: 62172\n",
      "Elapsed time:  00:00:54\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 408.30, [last 100]: 285.60, [all]: 314.00                       \n",
      "epsilon: 0.02, frames_total: 66255\n",
      "Elapsed time:  00:00:58\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 369.80, [last 100]: 277.75, [all]: 316.53                       \n",
      "epsilon: 0.02, frames_total: 69953\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 335.40, [last 100]: 265.63, [all]: 317.35                       \n",
      "epsilon: 0.02, frames_total: 73307\n",
      "Elapsed time:  00:01:04\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 404.70, [last 100]: 265.33, [all]: 320.97                       \n",
      "epsilon: 0.02, frames_total: 77354\n",
      "Elapsed time:  00:01:07\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 466.60, [last 100]: 271.83, [all]: 326.77                       \n",
      "epsilon: 0.02, frames_total: 82020\n",
      "Elapsed time:  00:01:12\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 362.90, [last 100]: 294.15, [all]: 328.16                       \n",
      "epsilon: 0.02, frames_total: 85649\n",
      "Elapsed time:  00:01:16\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 367.90, [last 100]: 297.32, [all]: 329.62                       \n",
      "epsilon: 0.02, frames_total: 89328\n",
      "Elapsed time:  00:01:19\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 445.70, [last 100]: 331.59, [all]: 333.75                       \n",
      "epsilon: 0.02, frames_total: 93785\n",
      "Elapsed time:  00:01:24\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 435.80, [last 100]: 374.12, [all]: 337.26                       \n",
      "epsilon: 0.02, frames_total: 98143\n",
      "Elapsed time:  00:01:28\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 480.70, [last 100]: 407.78, [all]: 342.03                       \n",
      "epsilon: 0.02, frames_total: 102950\n",
      "Elapsed time:  00:01:32\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 480.70, [last 100]: 415.02, [all]: 346.49                       \n",
      "epsilon: 0.02, frames_total: 107757\n",
      "Elapsed time:  00:01:36\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 319.00, [last 100]: 409.94, [all]: 345.63                       \n",
      "epsilon: 0.02, frames_total: 110947\n",
      "Elapsed time:  00:01:39\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 376.60, [last 100]: 414.06, [all]: 346.56                       \n",
      "epsilon: 0.02, frames_total: 114713\n",
      "Elapsed time:  00:01:42\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 371.70, [last 100]: 410.76, [all]: 347.30                       \n",
      "epsilon: 0.02, frames_total: 118430\n",
      "Elapsed time:  00:01:45\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 315.20, [last 100]: 395.62, [all]: 346.39                       \n",
      "epsilon: 0.02, frames_total: 121582\n",
      "Elapsed time:  00:01:47\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 377.30, [last 100]: 397.06, [all]: 347.24                       \n",
      "epsilon: 0.02, frames_total: 125355\n",
      "Elapsed time:  00:01:50\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 421.90, [last 100]: 402.46, [all]: 349.26                       \n",
      "epsilon: 0.02, frames_total: 129574\n",
      "Elapsed time:  00:01:53\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 414.70, [last 100]: 399.36, [all]: 350.97                       \n",
      "epsilon: 0.02, frames_total: 133721\n",
      "Elapsed time:  00:01:57\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 428.60, [last 100]: 398.64, [all]: 352.96                       \n",
      "epsilon: 0.02, frames_total: 138007\n",
      "Elapsed time:  00:02:01\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 317.20, [last 100]: 382.29, [all]: 352.07                       \n",
      "epsilon: 0.02, frames_total: 141179\n",
      "Elapsed time:  00:02:03\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 388.70, [last 100]: 373.09, [all]: 352.96                       \n",
      "epsilon: 0.02, frames_total: 145066\n",
      "Elapsed time:  00:02:06\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 396.30, [last 100]: 380.82, [all]: 353.99                       \n",
      "epsilon: 0.02, frames_total: 149029\n",
      "Elapsed time:  00:02:09\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 353.30, [last 100]: 378.49, [all]: 353.97                       \n",
      "epsilon: 0.02, frames_total: 152562\n",
      "Elapsed time:  00:02:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 471.60, [last 100]: 388.48, [all]: 356.64                       \n",
      "epsilon: 0.02, frames_total: 157278\n",
      "Elapsed time:  00:02:16\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 484.90, [last 100]: 405.45, [all]: 359.48                       \n",
      "epsilon: 0.02, frames_total: 162127\n",
      "Elapsed time:  00:02:21\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 417.72, [all]: 362.53                       \n",
      "epsilon: 0.02, frames_total: 167127\n",
      "Elapsed time:  00:02:25\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 425.53, [all]: 365.45                       \n",
      "epsilon: 0.02, frames_total: 172127\n",
      "Elapsed time:  00:02:30\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 484.10, [last 100]: 432.47, [all]: 367.92                       \n",
      "epsilon: 0.02, frames_total: 176968\n",
      "Elapsed time:  00:02:34\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 333.70, [last 100]: 422.98, [all]: 367.22                       \n",
      "epsilon: 0.02, frames_total: 180305\n",
      "Elapsed time:  00:02:37\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 368.57\n",
      "Average reward (last 100 episodes): 436.05\n",
      "Solved after 61 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "\n",
    "        return output2\n",
    "\n",
    "    \n",
    "# Memory for experience replay\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else: \n",
    "            self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1 ) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def select_action(self,state, epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values\n",
    "\n",
    "        \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "#         print(self.nn(state).size())\n",
    "#         print(action.unsqueeze(1).size())\n",
    "#         print(predicted_value.size())\n",
    "#         print(target_value.size())\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        \n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green', width=5)\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 0 ***                       \n",
      "Av.reward: [last 10]: 1.80, [last 100]: 0.18, [all]: 18.00                       \n",
      "epsilon: 0.87, frames_total: 18\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 10 ***                       \n",
      "Av.reward: [last 10]: 21.10, [last 100]: 2.29, [all]: 20.82                       \n",
      "epsilon: 0.58, frames_total: 229\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 20 ***                       \n",
      "Av.reward: [last 10]: 20.90, [last 100]: 4.38, [all]: 20.86                       \n",
      "epsilon: 0.39, frames_total: 438\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 30 ***                       \n",
      "Av.reward: [last 10]: 80.60, [last 100]: 12.44, [all]: 40.13                       \n",
      "epsilon: 0.09, frames_total: 1244\n",
      "Elapsed time:  00:00:01\n",
      "\n",
      "*** Episode 40 ***                       \n",
      "Av.reward: [last 10]: 233.50, [last 100]: 35.79, [all]: 87.29                       \n",
      "epsilon: 0.02, frames_total: 3579\n",
      "Elapsed time:  00:00:03\n",
      "\n",
      "*** Episode 50 ***                       \n",
      "Av.reward: [last 10]: 208.70, [last 100]: 56.66, [all]: 111.10                       \n",
      "epsilon: 0.02, frames_total: 5666\n",
      "Elapsed time:  00:00:05\n",
      "\n",
      "*** Episode 60 ***                       \n",
      "Av.reward: [last 10]: 176.90, [last 100]: 74.35, [all]: 121.89                       \n",
      "epsilon: 0.02, frames_total: 7435\n",
      "Elapsed time:  00:00:06\n",
      "\n",
      "*** Episode 70 ***                       \n",
      "Av.reward: [last 10]: 183.40, [last 100]: 92.69, [all]: 130.55                       \n",
      "epsilon: 0.02, frames_total: 9269\n",
      "Elapsed time:  00:00:08\n",
      "\n",
      "*** Episode 80 ***                       \n",
      "Av.reward: [last 10]: 235.30, [last 100]: 116.22, [all]: 143.48                       \n",
      "epsilon: 0.02, frames_total: 11622\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 90 ***                       \n",
      "Av.reward: [last 10]: 309.40, [last 100]: 147.16, [all]: 161.71                       \n",
      "epsilon: 0.02, frames_total: 14716\n",
      "Elapsed time:  00:00:13\n",
      "\n",
      "*** Episode 100 ***                       \n",
      "Av.reward: [last 10]: 283.60, [last 100]: 175.34, [all]: 173.78                       \n",
      "epsilon: 0.02, frames_total: 17552\n",
      "Elapsed time:  00:00:16\n",
      "\n",
      "*** Episode 110 ***                       \n",
      "Av.reward: [last 10]: 176.40, [last 100]: 190.87, [all]: 174.02                       \n",
      "epsilon: 0.02, frames_total: 19316\n",
      "Elapsed time:  00:00:17\n",
      "SOLVED! After 113 episodes \n",
      "\n",
      "*** Episode 120 ***                       \n",
      "Av.reward: [last 10]: 257.90, [last 100]: 214.57, [all]: 180.95                       \n",
      "epsilon: 0.02, frames_total: 21895\n",
      "Elapsed time:  00:00:20\n",
      "\n",
      "*** Episode 130 ***                       \n",
      "Av.reward: [last 10]: 372.70, [last 100]: 243.78, [all]: 195.59                       \n",
      "epsilon: 0.02, frames_total: 25622\n",
      "Elapsed time:  00:00:23\n",
      "\n",
      "*** Episode 140 ***                       \n",
      "Av.reward: [last 10]: 254.00, [last 100]: 245.83, [all]: 199.73                       \n",
      "epsilon: 0.02, frames_total: 28162\n",
      "Elapsed time:  00:00:26\n",
      "\n",
      "*** Episode 150 ***                       \n",
      "Av.reward: [last 10]: 312.60, [last 100]: 256.22, [all]: 207.21                       \n",
      "epsilon: 0.02, frames_total: 31288\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 160 ***                       \n",
      "Av.reward: [last 10]: 322.10, [last 100]: 270.74, [all]: 214.34                       \n",
      "epsilon: 0.02, frames_total: 34509\n",
      "Elapsed time:  00:00:31\n",
      "\n",
      "*** Episode 170 ***                       \n",
      "Av.reward: [last 10]: 370.60, [last 100]: 289.46, [all]: 223.48                       \n",
      "epsilon: 0.02, frames_total: 38215\n",
      "Elapsed time:  00:00:35\n",
      "\n",
      "*** Episode 180 ***                       \n",
      "Av.reward: [last 10]: 229.40, [last 100]: 288.87, [all]: 223.81                       \n",
      "epsilon: 0.02, frames_total: 40509\n",
      "Elapsed time:  00:00:37\n",
      "\n",
      "*** Episode 190 ***                       \n",
      "Av.reward: [last 10]: 231.90, [last 100]: 281.12, [all]: 224.23                       \n",
      "epsilon: 0.02, frames_total: 42828\n",
      "Elapsed time:  00:00:39\n",
      "\n",
      "*** Episode 200 ***                       \n",
      "Av.reward: [last 10]: 246.10, [last 100]: 277.37, [all]: 225.32                       \n",
      "epsilon: 0.02, frames_total: 45289\n",
      "Elapsed time:  00:00:41\n",
      "\n",
      "*** Episode 210 ***                       \n",
      "Av.reward: [last 10]: 394.30, [last 100]: 299.16, [all]: 233.33                       \n",
      "epsilon: 0.02, frames_total: 49232\n",
      "Elapsed time:  00:00:45\n",
      "\n",
      "*** Episode 220 ***                       \n",
      "Av.reward: [last 10]: 495.10, [last 100]: 322.88, [all]: 245.17                       \n",
      "epsilon: 0.02, frames_total: 54183\n",
      "Elapsed time:  00:00:49\n",
      "\n",
      "*** Episode 230 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 335.61, [all]: 256.20                       \n",
      "epsilon: 0.02, frames_total: 59183\n",
      "Elapsed time:  00:00:53\n",
      "\n",
      "*** Episode 240 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 360.21, [all]: 266.32                       \n",
      "epsilon: 0.02, frames_total: 64183\n",
      "Elapsed time:  00:00:57\n",
      "\n",
      "*** Episode 250 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 378.95, [all]: 275.63                       \n",
      "epsilon: 0.02, frames_total: 69183\n",
      "Elapsed time:  00:01:01\n",
      "\n",
      "*** Episode 260 ***                       \n",
      "Av.reward: [last 10]: 481.90, [last 100]: 394.93, [all]: 283.53                       \n",
      "epsilon: 0.02, frames_total: 74002\n",
      "Elapsed time:  00:01:06\n",
      "\n",
      "*** Episode 270 ***                       \n",
      "Av.reward: [last 10]: 430.70, [last 100]: 400.94, [all]: 288.96                       \n",
      "epsilon: 0.02, frames_total: 78309\n",
      "Elapsed time:  00:01:10\n",
      "\n",
      "*** Episode 280 ***                       \n",
      "Av.reward: [last 10]: 473.80, [last 100]: 425.38, [all]: 295.54                       \n",
      "epsilon: 0.02, frames_total: 83047\n",
      "Elapsed time:  00:01:15\n",
      "\n",
      "*** Episode 290 ***                       \n",
      "Av.reward: [last 10]: 255.00, [last 100]: 427.69, [all]: 294.15                       \n",
      "epsilon: 0.02, frames_total: 85597\n",
      "Elapsed time:  00:01:17\n",
      "\n",
      "*** Episode 300 ***                       \n",
      "Av.reward: [last 10]: 410.80, [last 100]: 444.16, [all]: 298.02                       \n",
      "epsilon: 0.02, frames_total: 89705\n",
      "Elapsed time:  00:01:22\n",
      "\n",
      "*** Episode 310 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 454.73, [all]: 304.52                       \n",
      "epsilon: 0.02, frames_total: 94705\n",
      "Elapsed time:  00:01:27\n",
      "\n",
      "*** Episode 320 ***                       \n",
      "Av.reward: [last 10]: 461.70, [last 100]: 451.39, [all]: 309.41                       \n",
      "epsilon: 0.02, frames_total: 99322\n",
      "Elapsed time:  00:01:31\n",
      "\n",
      "*** Episode 330 ***                       \n",
      "Av.reward: [last 10]: 481.30, [last 100]: 449.52, [all]: 314.61                       \n",
      "epsilon: 0.02, frames_total: 104135\n",
      "Elapsed time:  00:01:35\n",
      "\n",
      "*** Episode 340 ***                       \n",
      "Av.reward: [last 10]: 304.50, [last 100]: 429.97, [all]: 314.31                       \n",
      "epsilon: 0.02, frames_total: 107180\n",
      "Elapsed time:  00:01:39\n",
      "\n",
      "*** Episode 350 ***                       \n",
      "Av.reward: [last 10]: 162.40, [last 100]: 396.21, [all]: 309.98                       \n",
      "epsilon: 0.02, frames_total: 108804\n",
      "Elapsed time:  00:01:40\n",
      "\n",
      "*** Episode 360 ***                       \n",
      "Av.reward: [last 10]: 155.40, [last 100]: 363.56, [all]: 305.70                       \n",
      "epsilon: 0.02, frames_total: 110358\n",
      "Elapsed time:  00:01:42\n",
      "\n",
      "*** Episode 370 ***                       \n",
      "Av.reward: [last 10]: 157.90, [last 100]: 336.28, [all]: 301.72                       \n",
      "epsilon: 0.02, frames_total: 111937\n",
      "Elapsed time:  00:01:43\n",
      "\n",
      "*** Episode 380 ***                       \n",
      "Av.reward: [last 10]: 178.70, [last 100]: 306.77, [all]: 298.49                       \n",
      "epsilon: 0.02, frames_total: 113724\n",
      "Elapsed time:  00:01:45\n",
      "\n",
      "*** Episode 390 ***                       \n",
      "Av.reward: [last 10]: 214.60, [last 100]: 302.73, [all]: 296.34                       \n",
      "epsilon: 0.02, frames_total: 115870\n",
      "Elapsed time:  00:01:47\n",
      "\n",
      "*** Episode 400 ***                       \n",
      "Av.reward: [last 10]: 255.50, [last 100]: 287.20, [all]: 295.32                       \n",
      "epsilon: 0.02, frames_total: 118425\n",
      "Elapsed time:  00:01:49\n",
      "\n",
      "*** Episode 410 ***                       \n",
      "Av.reward: [last 10]: 182.00, [last 100]: 255.40, [all]: 292.57                       \n",
      "epsilon: 0.02, frames_total: 120245\n",
      "Elapsed time:  00:01:51\n",
      "\n",
      "*** Episode 420 ***                       \n",
      "Av.reward: [last 10]: 223.90, [last 100]: 231.62, [all]: 290.94                       \n",
      "epsilon: 0.02, frames_total: 122484\n",
      "Elapsed time:  00:01:53\n",
      "\n",
      "*** Episode 430 ***                       \n",
      "Av.reward: [last 10]: 437.00, [last 100]: 227.19, [all]: 294.32                       \n",
      "epsilon: 0.02, frames_total: 126854\n",
      "Elapsed time:  00:01:57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Episode 440 ***                       \n",
      "Av.reward: [last 10]: 394.10, [last 100]: 236.15, [all]: 296.59                       \n",
      "epsilon: 0.02, frames_total: 130795\n",
      "Elapsed time:  00:02:01\n",
      "\n",
      "*** Episode 450 ***                       \n",
      "Av.reward: [last 10]: 500.00, [last 100]: 269.91, [all]: 301.10                       \n",
      "epsilon: 0.02, frames_total: 135795\n",
      "Elapsed time:  00:02:06\n",
      "\n",
      "*** Episode 460 ***                       \n",
      "Av.reward: [last 10]: 482.70, [last 100]: 302.64, [all]: 305.04                       \n",
      "epsilon: 0.02, frames_total: 140622\n",
      "Elapsed time:  00:02:10\n",
      "\n",
      "*** Episode 470 ***                       \n",
      "Av.reward: [last 10]: 211.20, [last 100]: 307.97, [all]: 303.04                       \n",
      "epsilon: 0.02, frames_total: 142734\n",
      "Elapsed time:  00:02:12\n",
      "\n",
      "*** Episode 480 ***                       \n",
      "Av.reward: [last 10]: 300.40, [last 100]: 320.14, [all]: 302.99                       \n",
      "epsilon: 0.02, frames_total: 145738\n",
      "Elapsed time:  00:02:15\n",
      "\n",
      "*** Episode 490 ***                       \n",
      "Av.reward: [last 10]: 416.30, [last 100]: 340.31, [all]: 305.30                       \n",
      "epsilon: 0.02, frames_total: 149901\n",
      "Elapsed time:  00:02:19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Average reward: 306.85\n",
      "Average reward (last 100 episodes): 351.88\n",
      "Solved after 113 episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 500\n",
    "gamma = 1\n",
    "\n",
    "hidden_layer = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "# stable vs faster convergence trade-off\n",
    "update_target_frequency = 100\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "report_interval = 10\n",
    "score_to_solve = 195\n",
    "\n",
    "clip_error = True\n",
    "####################\n",
    "\n",
    "number_of_inputs = env.observation_space.shape[0]\n",
    "number_of_outputs = env.action_space.n\n",
    "\n",
    "def calculate_epsilon(steps_done):\n",
    "    epsilon = egreedy_final + (egreedy - egreedy_final) * \\\n",
    "              math.exp(-1. * steps_done / egreedy_decay )\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(number_of_inputs,hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer,number_of_outputs)\n",
    "\n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "\n",
    "        return output2\n",
    "\n",
    "    \n",
    "# Memory for experience replay\n",
    "class ExperienceReplay(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition)\n",
    "        else: \n",
    "            self.memory[self.position] = transition\n",
    "        self.position = (self.position + 1 ) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return zip(*random.sample(self.memory, batch_size))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class QNet_Agent(object):\n",
    "    def __init__(self):\n",
    "        self.nn = NeuralNetwork().to(device)\n",
    "        self.target_nn = NeuralNetwork().to(device)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        #self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(), lr=learning_rate)\n",
    "        #self.optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Update params target net\n",
    "        self.update_target_counter = 0\n",
    "        \n",
    "        \n",
    "    def select_action(self,state, epsilon):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > epsilon:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action = torch.max(action_from_nn,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def optimize(self):\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(batch_size)\n",
    "        \n",
    "        state = Tensor(state).to(device)\n",
    "        new_state = Tensor(new_state).to(device)\n",
    "        reward = Tensor(reward).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        done = Tensor(done).to(device)\n",
    "\n",
    "        new_state_values = self.target_nn(new_state).detach()\n",
    "        max_new_state_values = torch.max(new_state_values, 1)[0]\n",
    "        target_value = reward + (1 - done) * gamma * max_new_state_values\n",
    "\n",
    "        \n",
    "        predicted_value = self.nn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "#         print(self.nn(state).size())\n",
    "#         print(action.unsqueeze(1).size())\n",
    "#         print(predicted_value.size())\n",
    "#         print(target_value.size())\n",
    "        \n",
    "        loss = self.loss_func(predicted_value, target_value)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if clip_error:\n",
    "            for param in self.nn.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.update_target_counter % update_target_frequency == 0:\n",
    "            self.target_nn.load_state_dict(self.nn.state_dict())\n",
    "            \n",
    "        self.update_target_counter += 1\n",
    "        #Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "qnet_agent = QNet_Agent()\n",
    "\n",
    "steps_total = []\n",
    "\n",
    "frames_total = 0 \n",
    "solved_after = 0\n",
    "solved = False\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    step = 0\n",
    "    #for step in range(100):\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        frames_total += 1\n",
    "        \n",
    "        epsilon = calculate_epsilon(frames_total)\n",
    "        \n",
    "        #action = env.action_space.sample()\n",
    "        action = qnet_agent.select_action(state, epsilon)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        qnet_agent.optimize()\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            \n",
    "            mean_reward_100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if (mean_reward_100 > score_to_solve and solved == False):\n",
    "                print(\"SOLVED! After %i episodes \" % i_episode)\n",
    "                solved_after = i_episode\n",
    "                solved = True\n",
    "            \n",
    "            if (i_episode % report_interval == 0):\n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"\\n*** Episode %i *** \\\n",
    "                      \\nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \\\n",
    "                      \\nepsilon: %.2f, frames_total: %i\" \n",
    "                  % \n",
    "                  ( i_episode,\n",
    "                    report_interval,\n",
    "                    sum(steps_total[-report_interval:])/report_interval,\n",
    "                    mean_reward_100,\n",
    "                    sum(steps_total)/len(steps_total),\n",
    "                    epsilon,\n",
    "                    frames_total\n",
    "                          ) \n",
    "                  )\n",
    "                  \n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "\n",
    "\n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\\nAverage reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "if solved:\n",
    "    print(\"Solved after %i episodes\" % solved_after)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
